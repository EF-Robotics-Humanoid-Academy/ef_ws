================================================================================
  FAQ  --  Soda Can Detection Script (soda_can_detect.py)
  Unitree G1 + SDK2 Python (VideoClient over network) + CLIP Zero-Shot
================================================================================


----------------------------------------------------------------------
Q1: What does this script do?
----------------------------------------------------------------------

It connects to a Unitree G1 robot over the network (e.g. eth0),
requests a single JPEG frame from the robot's head-mounted camera via
the Unitree SDK2 Python RPC ``VideoClient``, and classifies the image
as "soda can in view" or "soda can NOT in view" using CLIP zero-shot
classification.  No custom training data is required.


----------------------------------------------------------------------
Q2: How is the image captured?
----------------------------------------------------------------------

The script uses the Unitree SDK2 Python library (``unitree_sdk2py``).
The call chain is:

  1. ``ChannelFactoryInitialize(0, "eth0")``
     Initialises the CycloneDDS transport layer on the specified
     network interface.  Domain ID 0 means "real robot" (1 = sim).

  2. ``client = VideoClient()``
     Creates an RPC client that talks to the ``videohub`` service
     running on the robot's internal computer.

  3. ``client.SetTimeout(3.0)``  /  ``client.Init()``
     Configures the RPC timeout and registers the GetImageSample API
     (API ID 1001, version 1.0.0.1).

  4. ``code, data = client.GetImageSample()``
     Performs a single RPC call.  On success (code == 0), ``data`` is
     a list of byte values containing a raw JPEG image.

  5. ``cv2.imdecode(np.frombuffer(bytes(data), np.uint8), cv2.IMREAD_COLOR)``
     Decodes the JPEG bytes into a BGR numpy array for OpenCV / CLIP.

This means the script runs on your **development machine** (laptop,
desktop, Jetson, etc.) — NOT on the robot itself — as long as the
machine is connected to the robot's network.


----------------------------------------------------------------------
Q3: What network setup is required?
----------------------------------------------------------------------

The G1 exposes its services on a fixed Ethernet subnet:

  Robot IP  : 192.168.123.161  (internal compute)
  Your IP   : 192.168.123.x    (where x != 161, 164)
  Interface : typically ``eth0`` (the Ethernet port connected to the
              robot via a direct cable or switch)

Make sure your development machine's interface is configured with an
IP in the 192.168.123.0/24 range and can ping the robot.

The DDS middleware (CycloneDDS 0.10.2) handles discovery automatically
once the interface is correct.


----------------------------------------------------------------------
Q4: How do I run the script?
----------------------------------------------------------------------

From the ``g1/scripts/obj_detection/`` directory (or anywhere, using
the full path):

  # Basic: capture + classify, print results to console
  python soda_can_detect.py --iface eth0

  # Save the annotated image to a file
  python soda_can_detect.py --iface eth0 --save result.jpg

  # Display the image in an OpenCV window
  python soda_can_detect.py --iface eth0 --show

  # Both save and show, with a higher confidence threshold
  python soda_can_detect.py --iface eth0 --save result.jpg --show --threshold 0.7

  # Increase the RPC timeout if the robot is slow to respond
  python soda_can_detect.py --iface eth0 --timeout 5.0


----------------------------------------------------------------------
Q5: What are all the command-line arguments?
----------------------------------------------------------------------

  --iface IFACE       Network interface connected to the robot.
                      Default: eth0

  --threshold FLOAT   Minimum CLIP probability for "a soda can" to
                      count as detected.  Range [0, 1].
                      Default: 0.6

  --timeout FLOAT     RPC timeout in seconds for VideoClient.
                      Default: 3.0

  --save PATH         Save the annotated frame to this file path
                      (e.g. result.jpg).  Optional.

  --show              Open an OpenCV window to display the annotated
                      frame.  Press any key to close.  Optional.


----------------------------------------------------------------------
Q6: What does the output look like?
----------------------------------------------------------------------

Console output:

  Connecting to G1 via interface 'eth0' ...
  Captured frame: 1920x1080 pixels
  Loading CLIP model 'openai/clip-vit-base-patch32' on 'cuda' ...
  CLIP model loaded.

  Classification results:
    Detected  : True
    Confidence: 82.3%
    Label     : a soda can
    Score for 'a soda can': 82.3%
    Score for 'no soda can, an empty scene': 17.7%

  Tip: use --save result.jpg or --show to see the annotated image.

If --save is used, the saved JPEG will have green/red status text and
a confidence percentage overlaid in the top-left corner.


----------------------------------------------------------------------
Q7: What is CLIP and why was it chosen?
----------------------------------------------------------------------

CLIP (Contrastive Language-Image Pre-training) is a neural network by
OpenAI trained on ~400 million image-text pairs.  It maps images and
text descriptions into a shared embedding space, enabling zero-shot
image classification: you provide text labels and an image, and CLIP
returns the probability that each label describes the image.

Why CLIP for this proof of concept:

  1. ZERO-SHOT -- no training data needed.  A custom CNN would require
     collecting and labelling hundreds of soda-can images.
  2. ALREADY INSTALLED -- torch and transformers are in requirements.txt.
  3. RUNS LOCALLY -- no API key, no internet at inference time, no cost.
  4. FAST ENOUGH -- ~35-70 ms/frame on CUDA, 200-500 ms on CPU.
  5. EASY TO EXTEND -- add more labels to CANDIDATE_LABELS for other
     objects without retraining.


----------------------------------------------------------------------
Q8: How does the CLIP classification work step by step?
----------------------------------------------------------------------

  1. The captured BGR frame is converted to RGB (CLIP expects RGB).
  2. Two text prompts are tokenised:
       "a soda can"
       "no soda can, an empty scene"
  3. CLIP's Vision Transformer (ViT-B/32) encodes the image into a
     512-dimensional vector.
  4. CLIP's text transformer encodes each prompt into the same 512-d
     space.
  5. Cosine similarity is computed between the image vector and each
     text vector, then softmax produces probabilities summing to 1.0.
  6. If P("a soda can") >= threshold (default 0.60) -> DETECTED.


----------------------------------------------------------------------
Q9: What model is downloaded and how big is it?
----------------------------------------------------------------------

Model: openai/clip-vit-base-patch32 (HuggingFace Hub)

  pytorch_model.bin        ~605 MB
  config.json + tokenizer  ~1 MB
  Total                    ~606 MB (one-time download)

Cached at ~/.cache/huggingface/hub/ and reused on subsequent runs.

To pre-download before going offline:

  python -c "from transformers import CLIPModel, CLIPProcessor; \
    CLIPModel.from_pretrained('openai/clip-vit-base-patch32'); \
    CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')"


----------------------------------------------------------------------
Q10: Why use the SDK2 VideoClient instead of pyrealsense2 directly?
----------------------------------------------------------------------

The previous version of this module used ``pyrealsense2`` to access the
RealSense D435i camera directly over USB.  That only works if the
script runs on the same machine that the camera is physically plugged
into (the robot's onboard Jetson or internal compute board).

The SDK2 ``VideoClient`` approach is better for development because:

  - The script runs on your laptop / desktop, not on the robot.
  - You connect over Ethernet (--iface eth0) from anywhere on the
    robot's LAN.
  - No USB pass-through, no SSH, no VNC needed to get an image.
  - Same RPC mechanism used by all other SDK2 clients (LocoClient,
    etc.) so it fits the established workflow.

Trade-off: VideoClient gives you one JPEG at a time (snapshot),
whereas pyrealsense2 gives a continuous frame stream.  For a PoC
single-frame classifier this is fine.


----------------------------------------------------------------------
Q11: What does "error code 3104" or a non-zero code from
     GetImageSample mean?
----------------------------------------------------------------------

Known error codes:

  0    : Success.
  3104 : Connection stall / timeout.  The videohub service on the
         robot did not respond in time.  Common causes:
           - Wrong network interface (try --iface enp1s0 or similar)
           - Robot not powered on or not fully booted
           - Firewall blocking DDS multicast traffic
           - Another client already holding the video session

If you see a non-zero code:
  1. Verify your network: ``ping 192.168.123.161``
  2. Check the interface name: ``ip link show``
  3. Increase the timeout: ``--timeout 10.0``
  4. Restart the robot if the videohub service is stuck.


----------------------------------------------------------------------
Q12: Why is this a single script instead of a Python package?
----------------------------------------------------------------------

This is a proof-of-concept.  A single file is easier to copy to any
machine, run, and iterate on.  There are no inter-module dependencies
to manage and no __init__.py / setup.py boilerplate.

If the PoC graduates to production, splitting into a camera module,
classifier module, and CLI entry point (as was done in the earlier
package-based version) would be a reasonable next step.


----------------------------------------------------------------------
Q13: What are the software dependencies?
----------------------------------------------------------------------

All are already in the workspace requirements.txt:

  unitree_sdk2py   Unitree SDK2 Python bindings (editable install)
  cyclonedds       DDS middleware (0.10.2, pulled in by SDK)
  torch            PyTorch runtime for the CLIP model
  transformers     HuggingFace library (CLIPModel, CLIPProcessor)
  opencv-python    JPEG decoding, image annotation, display
  numpy            Array operations

No ROS installation is required.


----------------------------------------------------------------------
Q14: What hardware is needed?
----------------------------------------------------------------------

  Robot side:
    - Unitree G1 with head-mounted Intel RealSense D435i
    - Robot powered on, internal videohub service running

  Development machine:
    - Ethernet connection to the robot's 192.168.123.0/24 subnet
    - Python 3.10+ with the dependencies above
    - GPU recommended (NVIDIA with CUDA) but CPU works too

Approximate inference time:

  Device                    Time        Notes
  -------------------------+-----------+-------------------------------
  Desktop GPU (RTX 3060+)   15-25 ms   Best for iteration
  Jetson Xavier NX (CUDA)   35-70 ms   On-robot if needed later
  Modern x86 CPU            100-200 ms Viable for single snapshots
  ARM CPU (no GPU)          200-500 ms Slow but functional


----------------------------------------------------------------------
Q15: What is the confidence threshold and how should I tune it?
----------------------------------------------------------------------

The threshold (default 0.60) is the minimum softmax probability for
the "a soda can" label to report a positive detection.

  Lower (e.g. 0.40) = more sensitive, more false positives
  Higher (e.g. 0.80) = less sensitive, fewer false positives

If the detector misses real soda cans -> lower the threshold.
If it fires on random objects    -> raise the threshold.

You can also refine the text prompts in CANDIDATE_LABELS (see Q19).


----------------------------------------------------------------------
Q16: Can I detect other objects besides soda cans?
----------------------------------------------------------------------

Yes.  Edit the CANDIDATE_LABELS list near the top of the script:

  CANDIDATE_LABELS = ["a soda can", "a water bottle", "a coffee mug", "nothing"]

CLIP returns a probability for each label.  The current logic checks
only the first label for the "detected" boolean; you would extend the
classify_frame() return value for multi-class detection.


----------------------------------------------------------------------
Q17: Why not use the OpenAI GPT-4 Vision API?
----------------------------------------------------------------------

  - Requires an API key and internet on the development machine
  - Costs ~$0.01-0.03 per image
  - 1-3 seconds round-trip latency per image
  - External service dependency

CLIP runs locally, instantly, for free.  GPT-4V could be used as a
"second opinion" for ambiguous frames if needed later.


----------------------------------------------------------------------
Q18: Why not train a custom CNN or use YOLOv8?
----------------------------------------------------------------------

Custom CNN:
  - Needs hundreds of labelled soda-can images + a training pipeline
  - Overfitting risk with small datasets
  - Retraining needed for each new object

YOLOv8:
  - COCO has "bottle" but not "soda can"
  - Fine-tuning needs annotated bounding-box data
  - Overkill for binary classification (no localisation needed here)

Both are good upgrade paths for production; overkill for a PoC.


----------------------------------------------------------------------
Q19: Can I improve accuracy by changing the text prompts?
----------------------------------------------------------------------

Yes.  CLIP accuracy is sensitive to prompt phrasing.  Some tips:

  - Be specific: "a red Coca-Cola soda can on a table" often works
    better than just "a soda can".
  - Make the negative prompt descriptive too: "an indoor scene with
    no drinks or cans visible" is more informative than "nothing".
  - Add more negative prompts to reduce false positives:
      ["a soda can",
       "a plastic water bottle",
       "an empty room",
       "a person with no drink"]

Prompt engineering is the cheapest way to improve CLIP accuracy.


----------------------------------------------------------------------
Q20: What are the known limitations?
----------------------------------------------------------------------

  1. SINGLE FRAME -- captures one JPEG per run (no continuous stream).
     Run the script in a loop or switch to pyrealsense2 for streaming.

  2. CLASSIFICATION ONLY -- no bounding box or pixel-level location.

  3. BINARY SOFTMAX -- any cylindrical metallic object (beer can, tin)
     may look "more like a soda can than an empty scene" to CLIP.

  4. CLIP IS GENERAL-PURPOSE -- edge cases (crushed cans, partial
     occlusion, low light) may produce low confidence.

  5. FIRST-RUN DOWNLOAD -- CLIP weights (~606 MB) must be downloaded
     once, requiring internet access.

  6. VIDEOHUB STABILITY -- some users report the robot's videohub
     service stalling (error code 3104).  A retry or robot restart
     usually resolves this.

  7. JPEG QUALITY -- the VideoClient returns a single JPEG whose
     resolution and compression level are controlled by the robot's
     firmware, not by this script.


----------------------------------------------------------------------
Q21: What are sensible next steps beyond this PoC?
----------------------------------------------------------------------

  1. Loop mode -- wrap capture + classify in a loop with a short sleep
     to get pseudo-live detection.

  2. Temporal smoothing -- majority vote over the last N frames to
     eliminate flickering.

  3. Fine-tune CLIP or train a small CNN on real images captured from
     the robot's own camera in its operating environment.

  4. Switch to a detection model (YOLOv8, OWL-ViT) for bounding boxes
     if you need to localise the can for grasping.

  5. Use depth data (RealSense D435i) to estimate the distance to the
     detected soda can.

  6. Integrate with LocoClient to make the robot walk toward and pick
     up the detected can.


----------------------------------------------------------------------
Q22: File layout
----------------------------------------------------------------------

  g1/scripts/obj_detection/
      soda_can_detect.py    Single self-contained script.
                            Camera capture (SDK2 VideoClient) +
                            CLIP classification + CLI.
      FAQ.txt               This file.


================================================================================
  End of FAQ
================================================================================
