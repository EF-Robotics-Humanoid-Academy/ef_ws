================================================================================
CODEX PROMPTS — Unitree G1 Robot Scripts
================================================================================

These prompts are designed to be fed to Codex (or Claude Code) sequentially.
Prompt 0 must be run first so the agent understands the codebase.
Prompts 1-6 are independent and can be run in any order after prompt 0.


================================================================================
PROMPT 0 — Understand Repository Structure
================================================================================

Read and understand the full structure of this Unitree G1 robot codebase.
Do NOT write any code. Only read files and report back.

REPOSITORY ROOT: /home/ag/ef_academy/
GIT ROOT:        /home/ag/ef_academy/
WORKSPACE:       academy_content/ef_ws/
G1 SCRIPTS:      academy_content/ef_ws/g1/scripts/

DIRECTORY LAYOUT (g1/scripts/):

  basic/
    g1_loco_client_example.py   — Interactive menu: damp, stand, move, wave, shake
    g1_hl_motion_sequence.py    — Walk N meters, wave hand, turn, walk again (IMU-corrected)
    g1_hl_gait_measure.py       — Measure step count + distance via rt/sportmodestate
    test_finger_motion.py       — Dex3 finger-by-finger test (rt/dex3/right/cmd)
    safety/
      hanger_boot_sequence.py   — Brings robot from Damp -> FSM-200 balanced stand
      keyboard_controller.py    — WASD teleop with pynput (real key-up events)
    audio/
      greeting.py               — Play WAV locally or on robot via AudioClient
      record_wav.py             — Record WAV with sounddevice
    headlight_client/
      headlight.py              — Set RGB headlight via AudioClient.LedControl()

  arm_motion/
    arm_motion.py               — Low-level arm SDK controller (rt/arm_sdk, LowCmd_)
                                  Reads steps.json with joint angles, ramps between poses
                                  Uses CRC, kp/kd PD control, cosine easing
    wave.py                     — Wave gesture using arm_motion.py + wave.json
    high_five.py                — High-level high-five via G1ArmActionClient (action ID 18)
    pick_and_place/
      arm_motion.py             — Copy of arm_motion for pick-and-place
      g1_pick_place_hardcoded.py

  navigation/
    obstacle_avoidance/
      create_map.py             — OccupancyGrid class (numpy int8 grid, world<->grid coords)
      path_planner.py           — A* search + Bresenham LOS smoothing + waypoint spacing
      obstacle_detection.py     — DDS subscriber for SportModeState_.range_obstacle[4]
      locomotion.py             — Proportional-control walk_to(x,y) with heading correction
      navigate.py               — Full orchestrator: SLAM map + A* + dynamic replanning
      map_viewer.py             — OpenCV real-time 2D visualisation of occupancy grid
      slam_map.py               — DDS subscribers: HeightMap_, SLAM odom, SLAM pointcloud
      slam_service.py           — RPC client for slam_operate (start/end mapping, pose nav)
      slam_all.py, create_slam_map.py
      safety/                   — Local copy of hanger_boot_sequence + keyboard_controller
    slam/
      live_slam.py, live_slam_safe.py, slam_webapp_2.py
      livox_python.py, livox2_python.py
      utlidar_map_viewer.py, visualize_map.py
      run_geoff_gui.py, run_geoff_stack.py

  obj_detection/
    soda_can_detect.py          — CLIP zero-shot detection via VideoClient over network
    berxel_rgbd_test.py         — Berxel RGBD camera test
    clip_soda_can_detect 1.py   — Variant of CLIP detection

  other/
    ef_client.py                — Unified Robot wrapper (boot, sensors, motion, FSM, DDS discovery)
    basic_script.py             — Minimal walk-turn-walk (open-loop, no IMU)

SDK LOCATION:
  academy_content_2/academy_content/docs/repos/unitree_sdk2_python/

KEY SDK PATTERNS:
  - ChannelFactoryInitialize(0, "eth0")    — domain 0 = real robot
  - LocoClient (g1.loco.g1_loco_client)    — Move(vx,vy,vyaw), StopMove(), Damp()
  - VideoClient (go2.video.video_client)   — GetImageSample() -> (code, jpeg_bytes)
  - ChannelSubscriber("rt/sportmodestate", SportModeState_) — pose, IMU, range_obstacle
  - ChannelSubscriber("rt/lowstate", LowState_)             — per-joint motor state, IMU
  - ChannelPublisher("rt/arm_sdk", LowCmd_)                 — low-level arm joint control
  - G1ArmActionClient                                       — high-level arm actions (wave, high-five)
  - SlamOperateClient (rpc.client.Client)                   — SLAM mapping + pose navigation

DDS TOPICS:
  rt/sportmodestate       — SportModeState_ (position, velocity, imu_state, range_obstacle)
  rt/lowstate             — LowState_ (per-joint motor_state, imu_state)
  rt/arm_sdk              — LowCmd_ (arm joint commands with CRC)
  rt/utlidar/map_state    — HeightMap_ (SLAM occupancy heightmap)
  rt/utlidar/switch       — String_ ("ON"/"OFF" to enable lidar mapping)
  rt/utlidar/cloud_livox_mid360 — PointCloud2_ (raw lidar pointcloud)
  rt/unitree/slam_mapping/odom   — Odometry_ (SLAM pose)
  rt/unitree/slam_mapping/points — PointCloud2_ (SLAM accumulated points)
  rt/dex3/right/cmd       — HandCmd_ (Dex3 finger commands)
  rt/slam_info            — String_ (JSON SLAM status)
  rt/slam_key_info        — String_ (JSON SLAM key events)

IMPORTANT CONVENTIONS:
  - No ROS. Everything uses DDS via unitree_sdk2py directly.
  - hanger_boot_sequence() is the standard safe-boot entry point.
    It returns an initialized LocoClient in FSM-200 (balanced stand, ready to walk).
  - sys.path manipulation is used to import across directories (no packages).
  - Scripts accept --iface (network interface) as primary argument.

TASK: Read the following files and summarize what each one does, its inputs,
outputs, and key functions/classes. This is context for subsequent prompts.

Files to read:
  1. g1/scripts/basic/safety/hanger_boot_sequence.py
  2. g1/scripts/basic/g1_hl_motion_sequence.py
  3. g1/scripts/navigation/obstacle_avoidance/navigate.py
  4. g1/scripts/navigation/obstacle_avoidance/locomotion.py
  5. g1/scripts/navigation/obstacle_avoidance/obstacle_detection.py
  6. g1/scripts/navigation/obstacle_avoidance/create_map.py
  7. g1/scripts/navigation/obstacle_avoidance/path_planner.py
  8. g1/scripts/navigation/obstacle_avoidance/slam_map.py
  9. g1/scripts/navigation/obstacle_avoidance/slam_service.py
  10. g1/scripts/navigation/obstacle_avoidance/map_viewer.py
  11. g1/scripts/arm_motion/arm_motion.py
  12. g1/scripts/arm_motion/high_five.py
  13. g1/scripts/obj_detection/soda_can_detect.py
  14. g1/scripts/other/ef_client.py


================================================================================
PROMPT 1 — IMU-Feedback-Loop Corrected Locomotion
================================================================================

Create a script: g1/scripts/other/imu_walk.py

PURPOSE: Walk the G1 robot forward a specified distance with IMU yaw correction
so it walks in a straight line, then optionally turn a specified angle using
IMU feedback, then walk again. This is the IMU-corrected version of basic_script.py.

EXISTING REFERENCE CODE:
  - g1/scripts/basic/g1_hl_motion_sequence.py — has ImuCache, _send_for_duration(),
    _send_until_distance(), _send_until_yaw() which are the patterns to follow.
  - g1/scripts/basic/safety/hanger_boot_sequence.py — returns LocoClient in FSM-200.
  - g1/scripts/other/basic_script.py — the open-loop version (no IMU).

REQUIREMENTS:
  1. Boot via hanger_boot_sequence(iface=args.iface).
  2. Subscribe to "rt/lowstate" (LowState_) to get IMU rpy for yaw correction.
     Try unitree_hg.msg.dds_.LowState_ first, fall back to unitree_go.msg.dds_.LowState_.
  3. Subscribe to "rt/sportmodestate" (SportModeState_) for odometry position feedback.
  4. Implement a command loop that sends velocity commands at --cmd-hz (default 20 Hz):
     - While walking forward: use proportional yaw correction (Kp * yaw_error) to
       hold the initial heading. Monitor position to stop after --distance meters.
     - While turning: use proportional control to reach the target yaw angle,
       slowing down near the target. Stop when within --turn-tol degrees.
  5. Sequence: walk --distance meters, turn --turn-deg degrees, walk --distance meters.
  6. Accept CLI args: --iface, --distance (default 1.0), --speed (default 0.3),
     --turn-deg (default 90), --turn-rate (default 0.5), --cmd-hz (default 20),
     --yaw-kp (default 0.8), --turn-kp (default 2.0), --turn-tol (default 2.0).
  7. Use StopMove() between phases with 0.5s pauses.
  8. Wrap in try/finally to always call StopMove() on exit.

IMPLEMENTATION NOTES:
  - The IMU yaw is in msg.imu_state.rpy[2] (radians).
  - Position is in msg.position[0:2] (x, y in metres).
  - Wrap yaw angles to [-pi, pi] when computing errors.
  - Use math.hypot(dx, dy) to measure distance traveled from start position.
  - Client velocity command: client.Move(vx, vy, vyaw) or client.SetVelocity(vx, vy, vyaw).
  - Keep the code clean and self-contained (single file, ~150-200 lines max).


================================================================================
PROMPT 2 — SLAM Mapping and Path Planning
================================================================================

Create a script: g1/scripts/other/slam_nav.py

PURPOSE: A simplified SLAM-based navigation script that:
  (a) starts SLAM mapping via the slam_operate RPC service,
  (b) accumulates a 2D occupancy grid from the SLAM HeightMap_ topic,
  (c) lets the user specify a goal (x, y) via CLI args,
  (d) plans a path using A*,
  (e) executes the path with proportional locomotion control.

EXISTING REFERENCE CODE:
  - g1/scripts/navigation/obstacle_avoidance/slam_map.py
    SlamMapSubscriber subscribes to "rt/utlidar/map_state" (HeightMap_), converts
    to OccupancyGrid via to_occupancy(height_threshold).
    LidarSwitch publishes "ON"/"OFF" to "rt/utlidar/switch".
    SlamOdomSubscriber subscribes to SLAM odom (Odometry_) for pose.
  - g1/scripts/navigation/obstacle_avoidance/slam_service.py
    SlamOperateClient — RPC wrapper for slam_operate service.
    API IDs: 1801 (start_mapping), 1802 (end_mapping), 1804 (init_pose),
    1102 (pose_nav), 1201 (pause_nav), 1202 (resume_nav).
  - g1/scripts/navigation/obstacle_avoidance/create_map.py
    OccupancyGrid class with world_to_grid(), grid_to_world(), inflate(), save/load.
  - g1/scripts/navigation/obstacle_avoidance/path_planner.py
    astar(grid, start, goal) -> list of (row,col) or None.
    smooth_path(), grid_path_to_world_waypoints().
  - g1/scripts/navigation/obstacle_avoidance/locomotion.py
    Locomotion class with walk_to(target_x, target_y, timeout, check_obstacle).

REQUIREMENTS:
  1. Boot via hanger_boot_sequence(iface=args.iface).
  2. Initialize SLAM: publish "ON" to rt/utlidar/switch, optionally call
     slam_client.start_mapping().
  3. Subscribe to SLAM map (HeightMap_ on rt/utlidar/map_state) and convert
     to OccupancyGrid. Wait up to --slam-timeout seconds for first map.
  4. Subscribe to either SLAM odom or sport mode state for robot pose.
  5. Accept goal via --goal-x and --goal-y (metres, world frame).
  6. Plan path: inflate the grid, run A*, smooth, convert to waypoints.
  7. Execute: walk each waypoint using proportional heading+distance control
     (similar to Locomotion.walk_to pattern).
  8. On completion, call StopMove() and optionally save the map to --save-map.
  9. CLI args: --iface, --goal-x, --goal-y, --speed (default 0.3),
     --slam-timeout (default 6.0), --height-threshold (default 0.15),
     --inflation (default 3), --spacing (default 0.5), --save-map (path).

IMPLEMENTATION NOTES:
  - Import from the navigation/obstacle_avoidance/ modules by adding their
    directory to sys.path. Or re-implement the minimal needed logic inline.
  - The SlamMapSubscriber.to_occupancy() method returns (OccupancyGrid, SlamMapMeta).
  - Robot pose from SportModeState_: position[0]=x, position[1]=y, imu_state.rpy[2]=yaw.
  - The walk_to pattern: compute heading to target, turn proportionally, then
    move forward with heading correction. Stop when distance < tolerance.
  - Keep it self-contained (~200-300 lines) but you may import from the existing
    navigation modules if you add their directory to sys.path.


================================================================================
PROMPT 3 — Navigation With and Without Obstacle Avoidance
================================================================================

Create two scripts:

(A) g1/scripts/other/walk_to_goal.py — Navigation WITHOUT obstacle avoidance
(B) g1/scripts/other/walk_to_goal_safe.py — Navigation WITH obstacle avoidance

EXISTING REFERENCE CODE:
  - g1/scripts/navigation/obstacle_avoidance/locomotion.py
    Locomotion class: walk_to(target_x, target_y, final_yaw, timeout, check_obstacle).
    Proportional control: Kp_lin=0.8, Kp_yaw=1.5, position_tolerance=0.2m.
  - g1/scripts/navigation/obstacle_avoidance/obstacle_detection.py
    ObstacleDetector: subscribes to SportModeState_, exposes get_pose(), get_ranges(),
    front_blocked(), is_blocked(direction), get_obstacle_world_positions().
    range_obstacle[4]: FRONT=0, RIGHT=1, REAR=2, LEFT=3.
  - g1/scripts/navigation/obstacle_avoidance/navigate.py
    Full orchestrator with SLAM + A* + dynamic replanning (complex, 760 lines).
  - g1/scripts/navigation/obstacle_avoidance/create_map.py
    OccupancyGrid for marking and inflating obstacles.
  - g1/scripts/navigation/obstacle_avoidance/path_planner.py
    astar() + smooth_path() + grid_path_to_world_waypoints().

--- SCRIPT A: walk_to_goal.py (simple, no obstacles) ---

REQUIREMENTS:
  1. Boot via hanger_boot_sequence().
  2. Subscribe to rt/sportmodestate for pose (position + imu yaw).
  3. Accept --goal-x, --goal-y, --goal-yaw (optional final heading).
  4. Use proportional control to walk directly to goal:
     - Compute heading to target: atan2(dy, dx)
     - If heading error > threshold: turn in place (vx=0, vyaw=Kp*err)
     - Else: move forward with heading correction (vx=Kp*dist, vyaw=Kp*heading_err)
     - Stop when distance < 0.2m.
  5. Optionally turn to face --goal-yaw after arriving.
  6. CLI args: --iface, --goal-x, --goal-y, --goal-yaw, --speed, --timeout.
  7. ~80-120 lines.

--- SCRIPT B: walk_to_goal_safe.py (with obstacle avoidance) ---

REQUIREMENTS:
  1. Everything from Script A, plus:
  2. Use range_obstacle[4] from SportModeState_ for obstacle sensing.
  3. When front obstacle < 0.4m (stop_distance): halt, replan around it.
  4. Maintain a simple OccupancyGrid (10x10m, 0.1m resolution, centered on start).
  5. Mark obstacles detected by range sensors on the grid.
  6. When blocked: inflate grid, run A* from current position to goal,
     smooth path, convert to waypoints, walk each waypoint.
  7. If A* finds no path, print error and stop.
  8. Support --max-replans (default 10) for the replan loop.
  9. CLI args: all of Script A plus --inflation, --max-replans.
  10. ~200-250 lines.
  11. Import from navigation/obstacle_avoidance/ modules (add to sys.path):
      create_map.OccupancyGrid, path_planner.astar/smooth_path/grid_path_to_world_waypoints.

IMPLEMENTATION NOTES:
  - In Script B, the replan loop:
    while not goal_reached and replans < max_replans:
      mark obstacles from range sensors on grid
      inflate grid, run A*, smooth, waypoints
      for each waypoint: walk toward it, check obstacles each tick
      if blocked: replan (increment counter, break inner loop)
  - Robot pose from SportModeState_: position[0]=x, position[1]=y, imu_state.rpy[2]=yaw.
  - Always wrap in try/finally with StopMove().


================================================================================
PROMPT 4 — Visualization of Robot Data
================================================================================

Create a script: g1/scripts/other/robot_dashboard.py

PURPOSE: A terminal-based dashboard that subscribes to all available DDS topics
and displays live robot telemetry. No motion commands — read-only monitoring.

EXISTING REFERENCE CODE:
  - g1/scripts/other/ef_client.py
    Robot class with DDS discovery (DdsDiscovery using cyclonedds builtins),
    topic auto-subscription, ImuData, TopicStats, get_sport_state(),
    get_imu_data(), fsm_id(), fsm_mode().
  - g1/scripts/navigation/obstacle_avoidance/obstacle_detection.py
    ObstacleDetector pattern for subscribing to SportModeState_.
  - g1/scripts/navigation/obstacle_avoidance/slam_map.py
    SlamInfoSubscriber for rt/slam_info and rt/slam_key_info.

REQUIREMENTS:
  1. Initialize DDS with ChannelFactoryInitialize(0, iface).
  2. Subscribe to these topics and display their data:
     a. rt/sportmodestate (SportModeState_):
        - Position (x, y, z)
        - Velocity (vx, vy, vz)
        - IMU RPY (roll, pitch, yaw in degrees)
        - range_obstacle[4] (front, right, rear, left distances)
        - gait_type, mode
     b. rt/lowstate (LowState_):
        - IMU rpy, gyroscope, accelerometer
        - Temperature
        - Motor states for key joints (sample a few: legs, arms)
     c. FSM state via LocoClient RPC:
        - FSM ID (from ROBOT_API_ID_LOCO_GET_FSM_ID)
        - FSM Mode (from ROBOT_API_ID_LOCO_GET_FSM_MODE)
     d. SLAM info (rt/slam_info, rt/slam_key_info) if available
  3. Optionally attempt DDS topic discovery using cyclonedds builtins
     (DomainParticipant + BuiltinDataReader for BuiltinTopicDcpsTopic)
     to list ALL discovered DDS topics and their types.
  4. Display format: clear screen and redraw every 0.5s (use ANSI escape codes
     or simple print with os.system('clear')). Show a compact table with:
     - Section headers: [POSE], [IMU], [OBSTACLES], [FSM], [SLAM], [DDS TOPICS]
     - Values aligned in columns
     - Staleness indicator (show "STALE" if no update in >1s)
  5. CLI args: --iface (default eth0), --rate (display refresh Hz, default 2),
     --discover (enable DDS topic discovery), --no-lowstate (skip rt/lowstate).
  6. Exit cleanly on Ctrl+C.
  7. ~200-250 lines.

IMPLEMENTATION NOTES:
  - LowState_ type: try unitree_hg.msg.dds_.LowState_ first, fallback to
    unitree_go.msg.dds_.LowState_.
  - FSM query: create LocoClient, SetTimeout(5.0), Init(). Then call
    _Call(ROBOT_API_ID_LOCO_GET_FSM_ID, "{}") -> (code, json_str).
    Parse json_str to get {"data": <int>}.
  - DDS discovery requires: from cyclonedds.domain import DomainParticipant;
    from cyclonedds.builtin import BuiltinDataReader, BuiltinTopicDcpsTopic.
    Poll with reader.read(64) to get topic_name + type_name.
  - For the SLAM info topics, subscribe to String_ on rt/slam_info and
    rt/slam_key_info. Display the raw JSON string (truncated to 80 chars).
  - Thread safety: use threading.Lock for all cached state.
  - Do NOT send any motion commands. This script is purely observational.


================================================================================
PROMPT 5 — High-Level Arm Motion
================================================================================

Create a script: g1/scripts/other/arm_demo.py

PURPOSE: Demonstrate arm motion control on the G1, supporting both high-level
(G1ArmActionClient) and low-level (rt/arm_sdk LowCmd_) arm control.

EXISTING REFERENCE CODE:
  - g1/scripts/arm_motion/arm_motion.py
    ArmSdkController class: publishes to "rt/arm_sdk" (LowCmd_ with CRC).
    JOINT_INDEX maps joint names to motor indices per arm:
      Left:  shoulder_pitch=15, shoulder_roll=16, shoulder_yaw=17, elbow=18,
             wrist_roll=19, wrist_pitch=20, wrist_yaw=21
      Right: shoulder_pitch=22, shoulder_roll=23, shoulder_yaw=24, elbow=25,
             wrist_pitch=26, wrist_roll=27, wrist_yaw=28
    WAIST_YAW_IDX=12, NOT_USED_IDX=29 (set q=1 to enable arm SDK).
    Reads steps.json: {"arm","cmd_hz","kp","kd","steps":[{"name","duration","hold","angles"}]}.
    ramp_to_pose(pose, duration, easing) — cosine easing or linear interpolation.
    hold_pose(pose, hold_s) — maintain position for duration.
    seed_from_lowstate() — read current joint positions from rt/lowstate to avoid jumps.
  - g1/scripts/arm_motion/high_five.py
    G1ArmActionClient: ExecuteAction(18) = high five, ExecuteAction(99) = release arm.
  - g1/scripts/arm_motion/wave.py
    Uses arm_motion.py with wave.json steps file.

REQUIREMENTS:
  1. Support two modes via --mode argument:
     a. "high-level" — uses G1ArmActionClient for built-in gestures.
     b. "low-level" — uses ArmSdkController for custom joint poses.

  2. High-level mode (--mode high-level):
     - Accept --action-id (int) to execute a specific arm action.
     - Known action IDs: 18=high-five, 99=release. Discover others by trying.
     - Hold the pose for --hold seconds (default 3.0), then release (action 99).

  3. Low-level mode (--mode low-level):
     - Accept --steps (path to steps.json file) OR inline --pose arguments.
     - If --steps is given, load and execute the step sequence (same as arm_motion.py).
     - If --pose is given as "joint=angle,joint=angle" (e.g., "shoulder_pitch=-45,elbow=30"),
       ramp to that single pose over --duration seconds, hold for --hold seconds.
     - Support --arm (left/right, default right), --kp (default 40), --kd (default 1),
       --cmd-hz (default 50), --easing (linear/smooth, default smooth).
     - Always seed_from_lowstate() first to read current positions and avoid jumps.

  4. Provide a set of built-in demo poses (no steps.json needed):
     --demo wave    — shoulder_pitch=-60, elbow=45, wrist_pitch=-30 (wave-like)
     --demo reach   — shoulder_pitch=-90, elbow=0 (arm straight forward)
     --demo salute  — shoulder_pitch=-90, shoulder_roll=10, elbow=90

  5. CLI args: --iface, --mode, --arm, --action-id, --steps, --pose, --demo,
     --hold, --duration, --kp, --kd, --cmd-hz, --easing, --no-seed.
  6. ~150-200 lines.

IMPLEMENTATION NOTES:
  - ChannelFactoryInitialize(0, iface) must be called before creating any client.
  - For low-level: create ArmSdkController(iface, arm, cmd_hz, kp, kd).
    Convert joint name -> index using JOINT_INDEX[arm][joint_name].
    Convert degrees to radians with math.radians().
    Build pose as list of (index, radians) tuples.
  - For high-level: create G1ArmActionClient(), SetTimeout(10), Init().
    Call ExecuteAction(action_id), sleep(hold), ExecuteAction(99).
  - The CRC is computed internally by ArmSdkController._apply_targets().
  - Import from arm_motion/ by adding its directory to sys.path.


================================================================================
PROMPT 6 — Object Detection (CLIP Approach)
================================================================================

Create a script: g1/scripts/other/detect_objects.py

PURPOSE: Generalized zero-shot object detection using CLIP. Extends the
soda_can_detect.py approach to detect arbitrary objects specified by the user.

EXISTING REFERENCE CODE:
  - g1/scripts/obj_detection/soda_can_detect.py
    capture_frame(iface) — connects via VideoClient, calls GetImageSample(),
    decodes JPEG -> BGR numpy array.
    load_clip(model_name, device) — loads CLIPModel + CLIPProcessor from HuggingFace.
    classify_frame(frame, model, processor, device, threshold) — runs zero-shot
    classification with candidate labels, returns {detected, confidence, label, scores}.
    CANDIDATE_LABELS = ["a soda can", "no soda can, an empty scene"].

REQUIREMENTS:
  1. Accept a list of target objects via --objects "soda can,water bottle,person,chair".
     Automatically create CLIP candidate labels:
       positive: "a {object}" for each object
       negative: "none of the specified objects, an empty scene"
  2. Camera capture:
     - Connect to the robot's camera via VideoClient (same as soda_can_detect.py).
     - ChannelFactoryInitialize(0, iface), VideoClient(), GetImageSample().
     - Decode JPEG to BGR numpy array via cv2.imdecode().
  3. CLIP classification:
     - Load openai/clip-vit-base-patch32 (or --model override).
     - Run zero-shot classification: compute similarity between the image and
       all candidate text labels. Softmax to get probabilities.
     - For each object, report: detected (score > threshold), confidence score.
  4. Support two modes:
     a. --mode single (default): capture one frame, classify, print results, exit.
     b. --mode continuous: capture frames in a loop (--interval seconds between captures),
        classify each, print live results. Optionally show annotated frame (--show).
  5. Annotated output:
     - Draw detected objects as text overlay on the frame (green=detected, red=not).
     - Show confidence percentages.
     - If --save: write annotated image to file.
     - If --show: display in OpenCV window.
  6. CLI args: --iface (default eth0), --objects (comma-separated, required),
     --threshold (default 0.5), --model (default openai/clip-vit-base-patch32),
     --mode (single/continuous), --interval (default 2.0), --save, --show,
     --timeout (VideoClient timeout, default 3.0).
  7. ~150-200 lines.

IMPLEMENTATION NOTES:
  - VideoClient is at unitree_sdk2py.go2.video.video_client (works for G1 too).
  - GetImageSample() returns (code, data). code=0 means success.
    data is a list/bytes of JPEG data. Decode:
      jpeg = np.frombuffer(bytes(data), dtype=np.uint8)
      frame = cv2.imdecode(jpeg, cv2.IMREAD_COLOR)
  - CLIP inference:
      inputs = processor(text=labels, images=rgb_image, return_tensors="pt", padding=True)
      outputs = model(**inputs)
      probs = outputs.logits_per_image.softmax(dim=1).cpu().numpy()[0]
  - Convert BGR->RGB before passing to CLIP: rgb = frame[..., ::-1].copy()
  - Dependencies: torch, transformers, opencv-python, numpy.
  - In continuous mode, the model should be loaded once and reused.
  - ChannelFactoryInitialize can only be called once per process.
